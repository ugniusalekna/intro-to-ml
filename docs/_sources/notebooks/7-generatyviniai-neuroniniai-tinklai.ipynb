{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generatyviniai neuroniniai tinklai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generatyviniai adversariniai tinklai (angl. generative adversarial networks, GANs) yra dar viena dirbtinių neuroninių tinklų architektūra, naudojama naujų duomenų (vaizdo, garso) generavimui. \n",
    "\n",
    "- Idėja buvo prisatyta 2014 m. mokslininko Ian Goodfellow ir jo kolegų."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_paper.png\" alt=\"gans-paper\" width=\"45%\">\n",
    "<p><strong>7.1 pav., Straipsnis, kuriame pristatyti GANs </strong></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iki GAN tinklų atsiradimo generatyviniai modeliai nė neegzistavo. Kai kurie eksperimentai buvo atliekami su autokoderiais, dažnai duodančiais neryškius vaizdus, artefaktus. \n",
    "\n",
    "- Bet žmonės jau žinojo, kaip sukurti galingus vaizdų klasifikatorius! 2012 m. pasirodęs *AlexNet* modelis buvo pirmasis modelis, sutriuškinęs savo konkurentus vaizdo klasifikavimo uždaviniuose. \n",
    "\n",
    "- Goodfellow idėja buvo, užuot kūrus galingą generatorių, paimti jau egzistuojančio klasifikatoriaus (kurį jis pavadino diskriminatoriumi) architektūrą bei jį panaudoti *apmokant kitą modelį*, atsakingą už naujų duomenų generavimą.\n",
    "\n",
    "- Pagrindinė GAN inovacija – generatoriaus užduotis ne tiesiogiai sukurti duomenis, panašius į apmokymo aibę (labai sunki užduotis), bet sukurti tokius duomenis, kurie galėtų apgauti diskriminatorių, klasifikuojant juos kaip tikrus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagrindiniai komponentai\n",
    "\n",
    "**Generatorius (G)**: Generatoriaus tinklo tikslas - sukurti duomenų pavyzdžius, kurie nesiskirtų nuo tikrų duomenų. Jis pradeda su atsitiktiniu triukšmo vektoriumi ir paverčia jį duomenų pavyzdžiu. Generatoriaus tikslas - sukurti išvestį, kuri būtų kuo artimesnė tikrajam duomenų pasiskirstymui.\n",
    "\n",
    "**Diskriminatorius (D)**: Diskriminatorius yra binarinis klasifikatorius, kuriuo bandoma atskirti tikruosius duomenų pavyzdžius nuo generatoriaus sukurtų pavyzdžių. Jo įvestis tikras arba sugeneruotas duomenų rinkinio pavyzdys, o išvestis – tikimybė, nurodanti, ar pavyzdys yra klasifikuojamsa kaip tikras, ar kaip netikras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarinio mokymosi procesas\n",
    "\n",
    "- GAN tinklo apmokymas yra minmax optimizavimo uždavinys tarp generatoriaus ir diskriminatoriaus\n",
    "\n",
    "- Mokymosi pradžioje generatorius pateikia akivaizdžiai netikrus duomenis, todėls diskriminatorius greitai išmoksta nustatyti, kad jie yra netikri\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_1.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- Tęsiantis mokymosi procesui, generatorius vis labiau artėja prie sugeneruotų duomenų, galinčių apgauti diskriminatorių.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_2.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- Galiausiai, jei generatoriaus apmokymas pavyksta, diskriminatorius vis prasčiau atskiria tikrą duomenų atvejį nuo netikro. Jis pradeda klasifikuoti netikrus duomenis kaip tikrus, ir jo tikslumas mažėja.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_3.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- GAN architektūros schema atrodo daugmaž taip:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_architecture.png\" alt=\"gans-architecture\" width=\"80%\">\n",
    "<p><strong>7.2 pav., GAN architektūros schema </strong></p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netikties funkcija\n",
    "\n",
    "- Straipsnyje, kuriame pristatyti GANs, netikties funkcija apibrėžiama formule:\n",
    "\n",
    "$$\n",
    "L(G, D) = \\frac{1}{m} \\sum_{i=1}^{m} [\\log D(x^{(i)})] + \\frac{1}{m} \\sum_{i=1}^{m} [\\log(1 - D(G(z^{(i)})))] \n",
    "$$\n",
    "\n",
    "- Generatorius ($G$) stengiasi minimizuoti šią funkciją, o diskriminatorius ($D$) stengiasi ją maksimizuoti (nes norime \"apgauti\" diskriminatorių). Tai yra kiek kitokia optimizavimo forma, nuo mums įprastos $\\min_{w} f(w)$:\n",
    "\n",
    "$$\n",
    "\\min_{G} \\max_{D} L(G, D)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [GAN] is the most exciting idea in the last 10 years in Machine Learning.\n",
    "> \n",
    "> — **Yann LeCun**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daugdaros didelio matmens erdvėse\n",
    "\n",
    "- Kiekvieną vaizdą galime įsivaizduoti kaip tašką didelio matmens erdvėje – kiekvieno pikselio vertė atitinka tašką ant atitinkamos dimensijos ašies. Pvz., turėdami vieno kanalo (*grayscale*) nuotrauką, susidedančią iš 3 pikselių, šių nuotraukų erdvę galime geometriškai pavaizduoti 3D kubu:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/manifold_hypercube.gif\" alt=\"manifold-hypercube\" width=\"65%\">\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "\n",
    "- Tuo tarpu, 256x256 vieno kanalo vaizdą galima pavaizduoti 65 536 matmenų hiperkube.\n",
    "\n",
    "- Didžioji dauguma tokio hiperkubo taškų yra triukšmingi, beprasmiai vaizdai. Reikšmingi vaizdai, pavyzdžiui, nuotraukos ar parašyti puslapiai, šioje erdvėje pasitaiko itin retai.\n",
    "\n",
    "- Daugdaros – tai mažesnio matavimo poerdviai aukšto matmens erdvėse, turintys mažiau laisvės laipsnių (t.y. gali būti atvaizduoti į mažesnio matavimo erdves). \n",
    "\n",
    "- Manoma, jog prasmingi vaizdai yra išsidėstę mažesnio matmens poerdviuose šioje didelio matmens erdvėje (hiperkube). Pavyzdžiui, vaizdų, kuriuose vaizduojamas žmogaus veidas su skirtingomis išraiškomis, rinkinys yra kažkur „veidų daugdaroje“, kadangi visų tokių nuotraukų pikselių pasiskirstymai turėtų būti bent kiek panašūs. Suradus tokias daugdaras bei judant jomis, galime matyti sklandžiai (tolydžiai) besikeičiančių vaizdų animacijas.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/manifold_transitions.gif\" alt=\"manifold-transitions\" width=\"65%\">\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "\n",
    "- Generatyviniai adversariniai tinklai išmoksta aproksimuoti šias daugdaras. Jie atvaizduoja mažo matmens erdvę (angl. latent space) į didelio matmens vaizdų erdvę, taip išmokdami daugdaros struktūrą. Turint daugdaros aproksimaciją, ją galima panaudoti \"vaikštant joje\", taip sugeneruojant realistiškai atrodančias nuotraukas, esančias šalia viena kitos (toje *latent space*) bei gaunant gražius netriukšmingus perėjimus.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/drag_gan.gif\" alt=\"drag-gan\" width=\"65%\">\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nors pirmųjų GAN'ų rezultatai nėra labai realistiški, tačiau pasigilinti į jų veikimo principą labai įdomu.\n",
    "\n",
    "- Straipsnyje *\"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\"* buvo pristatyti pirmieji bandymai išnaudoti *latent space* nuotraukų manipuliavimui. Atliktos įvairios aritmetinės projekcijos su vaizdų reprezentacija *latent* erdvėje, jų rezultatai yra visiškai kitokie, negu atlikus tuos pačius veiksmus *pixel-wise*.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gan_stuff.png\" alt=\"gan-on-faces\" width=\"85%\">\n",
    "<p><strong>7.3 pav., Aritmetinės operacijos tarp vaizdų </strong></p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "- Atliekant interpoliaciją tarp *latent space* vektorių, gaunamos ganėtinai realistiški perėjimai tarp skirtingų vaizdų. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gan_stuff_2.png\" alt=\"gan-on-faces\" width=\"85%\">\n",
    "<p><strong>7.4 pav., Interpoliacija tarp vaizdų </strong></p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacija PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kadangi jau žinote, kaip sukurti puikiai veikiantį klasifikatorių, tereikia jį adaptuoti taip, kad jis atliktų binarinio klasifikavimo uždavinį – prognozuotų, ar duotasis vaizdas yra tikras, ar ne.\n",
    "\n",
    "- Telieka aprašyti modelį, kuris bus atsakingas už naujų paveikslėlių generavimą.\n",
    "\n",
    "- Sukursime generatyvinį–adversarinį tinklą (GAN), kuris išmoks generuoti pasirinktos klasės vaizdus. Pritaikysime jį siekiant sugeneruoti dirbtinius skaitmenis iš `MNIST` duomenų rinkinio bei dirbtinius piešinėlius iš `QuickDraw` duomenų rinkinio.\n",
    "\n",
    "- Progreso saugojimui ir rezultatų vizualizavimui pasinaudosime įrankiu `SummaryWriter` iš `torch.utils.tensorboard`. \n",
    "\n",
    "- Treniruoti GAN architektūros modelius labai sudėtinga, dažnai susiduriama su modelio nestabilumais, todėl mūsų implementacija bus paremta straipsniu, kuriame ši architektūra buvo pristatyta [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434). Visi hiperparametrai yra paremti straipsnio autorių nustatytomis vertėmis.\n",
    "\n",
    "### Esminiai aspektai:\n",
    "\n",
    "1. **Duomenų apdorojimas**\n",
    "   - Kuomet duomenys paverčiami į tenzorius, vertes reikia normalizuoti tarp intervalo [-1, 1] reikšmių (įprastas normalizavimas yra tarp [0, 1] reišmių) (pagal straipsio implementaciją).\n",
    "\n",
    "2. **GAN Architektūra:**\n",
    "   - **Generatorius:** Jis bus atsakingas už naujų vaizdų generavimą. Architektūra susideda iš kelių dekonvoliucijos (angl. transposed convolution) sluoksnių (`torch.nn.ConvTranspose2d`), kad iš mažos dimensijos pradinio paveikslėlio (triukšmo) galėtume generuoti vaizdus didindami rezoliuciją, normalizavimo sluoksnių (`torch.nn.BatchNorm2d`), aktyvacijos funkcijos (`torch.nn.ReLU`). *pastaba:* paskutinyje sluoksnyje *batchnorm* nėra naudojama, o aktyvacijos funkcija *relu* pakeičiama į *tanh* (pagal straipsnio implementaciją).\n",
    "   \n",
    "   - **Diskriminatorius:** Jis bus naudojamas nustatyti, ar vaizdas yra tikras, ar ne. Architektūra susideda iš kelių konvoliucinių sluoksnių (`torch.nn.Conv2d`), normalizavimo sluoksnių (`torch.nn.BatchNorm2d`), aktyvacijos funkcijos (`torch.nn.LeakyReLU`). *pastaba:* *batchnorm* pirmame sluoksnyje nėra naudojamas; *leaky relu* aktyvacijos funkcijos argumentas 0.2 (pagal straipsnio implementaciją).\n",
    "\n",
    "   - **Modelių inicializavimas**. Visi modelio svoriai pradžioje inicializuojami iš normalaus pasiskirstymo (su vidurkiu 0 ir standartiniu nuokrypiu 0.02) (pagal straipsnio implementaciją).\n",
    "\n",
    "3. **Hiperparametrai ir Mokymosi Strategija:**\n",
    "   - `torch.optim.Adam` optimizatorius bus naudojamas tiek generatoriaus, tiek diskriminatoriaus apmokymui. *pastaba*: optimizatoriaus *learning rate* lygus 0.0002, parametro *beta_1* reikšmė – 0.5 (pagal straipsnio implementaciją).\n",
    "\n",
    "4. **Mokymo Ciklas:**\n",
    "   - Kiekvieno mokymo iteracijoje, generatorius generuos vaizdus, kuriuos diskriminatorius įvertins. Generatoriaus tikslas - apgauti diskriminatorių klasifikuojant netikrus vaizdus kaip tikrus.\n",
    "   - Anksčiau apibrėžtas optimizavimo uždavinys $\\min_{G} \\max_{D} L(G, D)$ treniravimo žingsnyje atliekamas du kartus – vieną kartą minimizuojant diskriminatoriaus netiktį ir kitą kartą maksimizuojant generatoriaus netikį. Kadangi funkcijos $L(G, D)$ antrasis dėmuo nepriklauso nuo diskriminatoriaus išvesties, šis dėmuo gali būti ignoruojamas. Praktikoje ši netiktis implementuojama atiliekant minimizavimo uždavinį abiem atvejais, pasinaudojant *binary cross entropy* netiktimi. Visa tai užrašoma kaip\n",
    "\n",
    "   ```\n",
    "   L_d = BCELoss(D(x), 1) + BCELoss(D(G(z)), 0)\n",
    "   L_g = BCELoss(D(G(z)), 1)\n",
    "\n",
    "   ```\n",
    "   - kintamasis `x` simbolizuoja tikrą vaizdą, kintamasis `z` – atisitiktinai sugeneruotą triukšmo tenzorių, kurį generatorius panaudoja netikro vaizdo sugeneravimui. Treniravimo metu siekiama minimizuoti reikšmes `L_d` ir `L_g`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "seed = 999\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install quickdraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cat drawings\n",
      "load complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.datasets as datasets\n",
    "from quickdraw import QuickDrawDataGroup\n",
    "\n",
    "\n",
    "def get_MNIST_data():\n",
    "    MNIST_train = datasets.MNIST(root='../data', train=True, download=True)\n",
    "    MNIST_val = datasets.MNIST(root='../data', train=False, download=True)\n",
    "\n",
    "    return MNIST_train, MNIST_val\n",
    "\n",
    "\n",
    "def load_quickdraw_data(classes, image_size, val_split=None, max_drawings_per_class=None):\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_dict = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        qdg = QuickDrawDataGroup(cls, max_drawings=max_drawings_per_class)\n",
    "        for drawing in qdg.drawings:\n",
    "            image = drawing.get_image().convert('L')\n",
    "            image = image.resize(image_size)\n",
    "            image_array = np.array(image)\n",
    "            image_array = 255 - image_array\n",
    "            images.append(image_array)\n",
    "            labels.append(label_dict[cls])\n",
    "\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    if val_split is not None:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, stratify=y, random_state=42)\n",
    "        X, y = (X_train, y_train), (X_val, y_val)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "CLASSES = [\n",
    "    \"cat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, activation=True, batch_norm=True, **kwargs):\n",
    "        layers = [nn.Conv2d(channels_in, channels_out, bias=not batch_norm, **kwargs)]\n",
    "        layers += [nn.BatchNorm2d(channels_out)] if batch_norm else []\n",
    "        layers += [nn.LeakyReLU(0.2)] if activation else []\n",
    "        super().__init__(*layers)\n",
    "        \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, hidden_layers):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.channels_in = channels_in\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        layers = [ConvBlock(channels_in, hidden_layers[0], kernel_size=4, stride=2, padding=1, \n",
    "                                 batch_norm=False)]\n",
    "        layers += [\n",
    "            ConvBlock(hidden_layers[i], hidden_layers[i+1], kernel_size=4, stride=2, padding=1)\n",
    "            for i in range(len(hidden_layers) - 1)\n",
    "        ]\n",
    "        layers += [ConvBlock(hidden_layers[-1], 1, kernel_size=4, stride=1, padding=0,\n",
    "                                  batch_norm=False, activation=False)]\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        x = x.reshape(-1)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "model = Discriminator(image_size=(64, 64), channels_in=1, hidden_layers=[16, 32, 64, 128])\n",
    "model.apply(weights_init)\n",
    "\n",
    "inp = torch.rand(1, 1, 64, 64, dtype=torch.float32)\n",
    "out = model(inp)\n",
    "\n",
    "print(inp.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 1, 1])\n",
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class DeconvBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, activation=True, batch_norm=True, **kwargs):\n",
    "        layers = [nn.ConvTranspose2d(channels_in, channels_out, bias=not batch_norm, **kwargs)]\n",
    "        layers += [nn.BatchNorm2d(channels_out)] if batch_norm else []\n",
    "        layers += [nn.ReLU()] if activation else []\n",
    "        super().__init__(*layers) \n",
    "        \n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_layers, channels_out, latent_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.channels_out = channels_out\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        layers = [DeconvBlock(latent_dim, hidden_layers[0], kernel_size=4, stride=1, padding=0)]\n",
    "        layers += [\n",
    "            DeconvBlock(hidden_layers[i], hidden_layers[i+1], kernel_size=4, stride=2, padding=1)\n",
    "            for i in range(len(hidden_layers) - 1)\n",
    "        ]\n",
    "        layers += [DeconvBlock(hidden_layers[-1], channels_out, kernel_size=4, stride=2, padding=1,\n",
    "                                      batch_norm=False, activation=False)]\n",
    "        \n",
    "        self.deconv_blocks = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.deconv_blocks(x)\n",
    "        return F.tanh(x)\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "model = Generator(hidden_layers=[1024, 512, 256, 128], channels_out=1, latent_dim=100)\n",
    "model.apply(weights_init)\n",
    "\n",
    "inp = torch.randn(1, 100, 1, 1, dtype=torch.float32)\n",
    "out = model(inp)\n",
    "\n",
    "print(inp.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, train_loader, optimizer_G, optimizer_D, \n",
    "              criterion, device, num_epochs=100, latent_dim=100, log_dir=\"../logs/\"):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "    writer = SummaryWriter(log_dir=log_dir + timestamp)\n",
    "\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    \n",
    "    fixed_noise = torch.randn(16, latent_dim, 1, 1, device=device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        running_loss_G = running_loss_D = 0.0\n",
    "\n",
    "        for real_images, _ in tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]', leave=False):\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "\n",
    "            noise = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)\n",
    "            \n",
    "            disc_real = discriminator(real_images)\n",
    "            d_loss_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "            \n",
    "            fake_images = generator(noise)\n",
    "            disc_fake = discriminator(fake_images)\n",
    "            d_loss_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "            \n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            \n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "\n",
    "            running_loss_D += d_loss.item()\n",
    "\n",
    "            # Train Generator\n",
    "\n",
    "            output = discriminator(fake_images)\n",
    "            g_loss = criterion(output, torch.ones_like(output))\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            running_loss_G += g_loss.item()\n",
    "\n",
    "        writer.add_scalar('Loss/Discriminator', running_loss_D / len(train_loader), epoch+1)\n",
    "        writer.add_scalar('Loss/Generator', running_loss_G / len(train_loader), epoch+1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake = generator(fixed_noise)\n",
    "            \n",
    "            img_batch = torch.cat((real_images[:16], fake), 0)\n",
    "            img_grid = make_grid(img_batch, nrow=img_batch.size(0) // 2, normalize=True, value_range=(-1, 1))\n",
    "                    \n",
    "            writer.add_image('Generated vs Real', img_grid, global_step=epoch+1)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'epoch': num_epochs,\n",
    "    }, log_dir + timestamp + \"/gan_state_dict.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class DoodlesDataset(Dataset):\n",
    "    def __init__(self, data, image_size=None, transform=None):\n",
    "        \n",
    "        self.images, self.labels = data\n",
    "        self.resize = T.Resize(image_size) if image_size else None\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = self.to_tensor(image)\n",
    "        \n",
    "        if self.resize:\n",
    "            image = self.resize(image)\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data, size=(32, 32), transform=None):\n",
    "        self.data = data\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.resize = T.Resize(size)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        image = self.to_tensor(image)\n",
    "        image = self.resize(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (64, 64)\n",
    "CHANNELS_IN = 1\n",
    "HIDDEN_LAYERS_D = [16, 32, 64, 128] # [64, 128, 256, 512]\n",
    "HIDDEN_LAYERS_G = [1024, 512, 256, 128] # [512, 256, 128, 64]\n",
    "LATENT_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "LEARNING_RATE = 0.0002\n",
    "BETA_1 = 0.5\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "DEVICE = 'mps'\n",
    "\n",
    "LOG_DIR = '../logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data = load_quickdraw_data(CLASSES, image_size=(128, 128))\n",
    "# train_data, _ = get_MNIST_data()\n",
    "\n",
    "train_dataset = DoodlesDataset(train_data, image_size=IMAGE_SIZE, transform=transform)\n",
    "# train_dataset = MNISTDataset(train_data, size=IMAGE_SIZE, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "device = torch.device(DEVICE)\n",
    "\n",
    "discriminator = Discriminator(image_size=IMAGE_SIZE, channels_in=CHANNELS_IN, hidden_layers=HIDDEN_LAYERS_D)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "generator = Generator(hidden_layers=HIDDEN_LAYERS_G, channels_out=CHANNELS_IN, latent_dim=LATENT_DIM)\n",
    "generator.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA_1, 0.999))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA_1, 0.999))\n",
    "\n",
    "train_gan(generator, discriminator, train_loader, optimizer_G, optimizer_D, \n",
    "          criterion, device, num_epochs=NUM_EPOCHS, latent_dim=LATENT_DIM, log_dir=LOG_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
