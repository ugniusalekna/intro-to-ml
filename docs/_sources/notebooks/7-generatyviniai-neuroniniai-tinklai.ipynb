{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generatyviniai neuroniniai tinklai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generatyviniai adversariniai tinklai (angl. generative adversarial networks, GANs) yra dar viena dirbtinių neuroninių tinklų architektūra, naudojama naujų duomenų (vaizdo, garso) generavimui. \n",
    "\n",
    "- Idėja buvo prisatyta 2014 m. mokslininko Ian Goodfellow ir jo kolegų."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_paper.png\" alt=\"gans-paper\" width=\"45%\">\n",
    "<p><strong>7.1 pav., Straipsnis, kuriame pristatyti GANs </strong></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iki GAN tinklų atsiradimo generatyviniai modeliai nė neegzistavo. Kai kurie eksperimentai buvo atliekami su autokoderiais, dažnai duodančiais neryškius vaizdus, artefaktus. \n",
    "\n",
    "- Bet žmonės jau žinojo, kaip sukurti galingus vaizdų klasifikatorius! 2012 m. pasirodęs *AlexNet* modelis buvo pirmasis modelis, sutriuškinęs savo konkurentus vaizdo klasifikavimo uždaviniuose. \n",
    "\n",
    "- Goodfellow idėja buvo, užuot kūrus galingą generatorių, paimti jau egzistuojančio klasifikatoriaus (kurį jis pavadino diskriminatoriumi) architektūrą bei jį panaudoti *apmokant kitą modelį*, atsakingą už naujų duomenų generavimą.\n",
    "\n",
    "- Pagrindinė GAN inovacija – generatoriaus užduotis ne tiesiogiai sukurti duomenis, panašius į apmokymo aibę (labai sunki užduotis), bet sukurti tokius duomenis, kurie galėtų apgauti diskriminatorių, klasifikuojant juos kaip tikrus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagrindiniai komponentai\n",
    "\n",
    "**Generatorius (G)**: Generatoriaus tinklo tikslas - sukurti duomenų pavyzdžius, kurie nesiskirtų nuo tikrų duomenų. Jis pradeda su atsitiktiniu triukšmo vektoriumi ir paverčia jį duomenų pavyzdžiu. Generatoriaus tikslas - sukurti išvestį, kuri būtų kuo artimesnė tikrajam duomenų pasiskirstymui.\n",
    "\n",
    "**Diskriminatorius (D)**: Diskriminatorius yra binarinis klasifikatorius, kuriuo bandoma atskirti tikruosius duomenų pavyzdžius nuo generatoriaus sukurtų pavyzdžių. Jo įvestis tikras arba sugeneruotas duomenų rinkinio pavyzdys, o išvestis – tikimybė, nurodanti, ar pavyzdys yra klasifikuojamsa kaip tikras, ar kaip netikras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarinio mokymosi procesas\n",
    "\n",
    "- GAN tinklo apmokymas yra minmax optimizavimo uždavinys tarp generatoriaus ir diskriminatoriaus\n",
    "\n",
    "- Mokymosi pradžioje generatorius pateikia akivaizdžiai netikrus duomenis, todėls diskriminatorius greitai išmoksta nustatyti, kad jie yra netikri\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_1.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- Tęsiantis mokymosi procesui, generatorius vis labiau artėja prie sugeneruotų duomenų, galinčių apgauti diskriminatorių.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_2.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- Galiausiai, jei generatoriaus apmokymas pavyksta, diskriminatorius vis prasčiau atskiria tikrą duomenų atvejį nuo netikro. Jis pradeda klasifikuoti netikrus duomenis kaip tikrus, ir jo tikslumas mažėja.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_3.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- GAN architektūros schema atrodo daugmaž taip:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_architecture.png\" alt=\"gans-architecture\" width=\"80%\">\n",
    "<p><strong>7.2 pav., GAN architektūros schema </strong></p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netikties funkcija\n",
    "\n",
    "- Straipsnyje, kuriame pristatyti GANs, netikties funkcija apibrėžiama formule:\n",
    "\n",
    "$$\n",
    "L(G, D) = \\frac{1}{m} \\sum_{i=1}^{m} [\\log D(x^{(i)})] + \\frac{1}{m} \\sum_{i=1}^{m} [\\log(1 - D(G(z^{(i)})))] \n",
    "$$\n",
    "\n",
    "- Generatorius ($G$) stengiasi minimizuoti šią funkciją, o diskriminatorius ($D$) stengiasi ją maksimizuoti (nes norime \"apgauti\" diskriminatorių). Tai yra kiek kitokia optimizavimo forma, nuo mums įprastos $\\min_{w} f(w)$:\n",
    "\n",
    "$$\n",
    "\\min_{G} \\max_{D} L(G, D)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [GAN] is the most exciting idea in the last 10 years in Machine Learning.\n",
    "> \n",
    "> — **Yann LeCun**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daugdaros didelio matmens erdvėse\n",
    "\n",
    "- Kiekvieną vaizdą galime įsivaizduoti kaip tašką didelio matmens erdvėje – kiekvieno pikselio vertė atitinka tašką ant atitinkamos dimensijos ašies. Pvz., turėdami vieno kanalo (*grayscale*) nuotrauką, susidedančią iš 3 pikselių, šių nuotraukų erdvę galime geometriškai pavaizduoti 3D kubu:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/manifold_hypercube.gif\" alt=\"manifold-hypercube\" width=\"65%\">\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "\n",
    "- Tuo tarpu, 256x256 vieno kanalo vaizdą galima pavaizduoti 65 536 matmenų hiperkube.\n",
    "\n",
    "- Didžioji dauguma tokio hiperkubo taškų yra triukšmingi, beprasmiai vaizdai. Reikšmingi vaizdai, pavyzdžiui, nuotraukos ar parašyti puslapiai, šioje erdvėje pasitaiko itin retai.\n",
    "\n",
    "- Daugdaros – tai mažesnio matavimo poerdviai aukšto matmens erdvėse, turintys mažiau laisvės laipsnių (t.y. gali būti atvaizduoti į mažesnio matavimo erdves). \n",
    "\n",
    "- Manoma, jog prasmingi vaizdai yra išsidėstę mažesnio matmens poerdviuose šioje didelio matmens erdvėje (hiperkube). Pavyzdžiui, vaizdų, kuriuose vaizduojamas žmogaus veidas su skirtingomis išraiškomis, rinkinys yra kažkur „veidų daugdaroje“, kadangi visų tokių nuotraukų pikselių pasiskirstymai turėtų būti bent kiek panašūs. Suradus tokias daugdaras bei judant jomis, galime matyti sklandžiai (tolydžiai) besikeičiančių vaizdų animacijas.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/manifold_transitions.gif\" alt=\"manifold-transitions\" width=\"65%\">\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "\n",
    "- Generatyviniai adversariniai tinklai išmoksta aproksimuoti šias daugdaras. Jie atvaizduoja mažo matmens erdvę (angl. latent space) į didelio matmens vaizdų erdvę, taip išmokdami daugdaros struktūrą. Turint daugdaros aproksimaciją, ją galima panaudoti \"vaikštant joje\", taip sugeneruojant realistiškai atrodančias nuotraukas, esančias šalia viena kitos (toje *latent space*) bei gaunant gražius netriukšmingus perėjimus.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/drag_gan.gif\" alt=\"drag-gan\" width=\"65%\">\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nors pirmųjų GAN'ų rezultatai nėra labai realistiški, tačiau pasigilinti į jų veikimo principą labai įdomu.\n",
    "\n",
    "- Straipsnyje *\"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\"* buvo pristatyti pirmieji bandymai išnaudoti *latent space* nuotraukų manipuliavimui. Atliktos įvairios aritmetinės projekcijos su vaizdų reprezentacija *latent* erdvėje, jų rezultatai yra visiškai kitokie, negu atlikus tuos pačius veiksmus *pixel-wise*.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gan_stuff.png\" alt=\"gan-on-faces\" width=\"85%\">\n",
    "<p><strong>7.3 pav., Aritmetinės operacijos tarp vaizdų </strong></p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "- Atliekant interpoliaciją tarp *latent space* vektorių, gaunamos ganėtinai realistiški perėjimai tarp skirtingų vaizdų. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gan_stuff_2.png\" alt=\"gan-on-faces\" width=\"85%\">\n",
    "<p><strong>7.4 pav., Interpoliacija tarp vaizdų </strong></p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacija PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bicycle drawings\n",
      "load complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from quickdraw import QuickDrawDataGroup\n",
    "\n",
    "def load_quickdraw_data(classes, image_size, val_split=None, max_drawings_per_class=None):\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_dict = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        qdg = QuickDrawDataGroup(cls, max_drawings=max_drawings_per_class)\n",
    "        for drawing in qdg.drawings:\n",
    "            image = drawing.get_image().convert('L')\n",
    "            image = image.resize(image_size)\n",
    "            image_array = np.array(image)\n",
    "            image_array = 255 - image_array\n",
    "            images.append(image_array)\n",
    "            labels.append(label_dict[cls])\n",
    "\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    if val_split is not None:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, stratify=y, random_state=42)\n",
    "        X, y = (X_train, y_train), (X_val, y_val)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "CLASSES = [\n",
    "    \"bicycle\",\n",
    "]\n",
    "\n",
    "train_data = load_quickdraw_data(CLASSES, image_size=(128, 128), max_drawings_per_class=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n",
      "torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, activation=True, batch_norm=True, **kwargs):\n",
    "        layers = [nn.Conv2d(channels_in, channels_out, bias=False, **kwargs)]\n",
    "        layers += [nn.BatchNorm2d(channels_out)] if batch_norm else []\n",
    "        layers += [nn.GELU()] if activation else []\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, hidden_layers):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.channels_in = channels_in\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        conv_layers = [ConvBlock(channels_in, hidden_layers[0], kernel_size=5, stride=2, padding=2, batch_norm=False)]\n",
    "        conv_layers += [\n",
    "            ConvBlock(hidden_layers[i], hidden_layers[i+1], kernel_size=4, stride=2, padding=1)\n",
    "            for i in range(len(hidden_layers) - 1)\n",
    "        ]\n",
    "        conv_layers += [ConvBlock(hidden_layers[-1], 1, kernel_size=4, stride=2, padding=0,\n",
    "                                  batch_norm=False, activation=False)]\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(*conv_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)        \n",
    "        return F.sigmoid(x)\n",
    "\n",
    "# Example usage\n",
    "model = Discriminator(image_size=(64, 64), channels_in=1, hidden_layers=[16, 32, 64, 128])\n",
    "inp = torch.rand(1, 1, 64, 64, dtype=torch.float32)\n",
    "out = model(inp)\n",
    "\n",
    "print(inp.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 1, 1])\n",
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeconvBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, activation=True, batch_norm=True, **kwargs):\n",
    "        layers = [nn.ConvTranspose2d(channels_in, channels_out, bias=False, **kwargs)]\n",
    "        layers += [nn.BatchNorm2d(channels_out)] if batch_norm else []\n",
    "        layers += [nn.GELU()] if activation else []\n",
    "        super().__init__(*layers) \n",
    "        \n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_layers, channels_out, latent_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.channels_out = channels_out\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        deconv_blocks = [DeconvBlock(latent_dim, hidden_layers[0], kernel_size=4, stride=1, padding=0)]\n",
    "        deconv_blocks += [\n",
    "            DeconvBlock(hidden_layers[i], hidden_layers[i+1], kernel_size=4, stride=2, padding=1)\n",
    "            for i in range(len(hidden_layers) - 1)\n",
    "        ]\n",
    "        deconv_blocks += [DeconvBlock(hidden_layers[-1], channels_out, kernel_size=4, stride=2, padding=1,\n",
    "                                      activation=False)]\n",
    "        \n",
    "        self.deconv_blocks = nn.Sequential(*deconv_blocks)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv_blocks(x)\n",
    "        return F.tanh(x)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Example usage\n",
    "model = Generator(hidden_layers=[1024, 512, 256, 128], channels_out=1, latent_dim=100)\n",
    "inp = torch.randn(1, 100, 1, 1, dtype=torch.float32)\n",
    "out = model(inp)\n",
    "\n",
    "print(inp.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, train_loader, optimizer_G, optimizer_D, \n",
    "              criterion, device, num_epochs=100, latent_dim=100, log_dir=\"../logs/\"):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "    writer = SummaryWriter(log_dir=log_dir + timestamp)\n",
    "\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "            \n",
    "    fixed_noise = torch.randn(4, latent_dim, 1, 1, device=device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        running_loss_G = running_loss_D = 0.0\n",
    "\n",
    "        for real_images, _ in tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]', leave=False):\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            # Train Discriminator max log(D(z)) + log(1 - D(G(z)))\n",
    "\n",
    "            noise = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)\n",
    "            \n",
    "            disc_real = discriminator(real_images).reshape(-1)\n",
    "            d_loss_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "            \n",
    "            fake_images = generator(noise)\n",
    "            disc_fake = discriminator(fake_images).reshape(-1)\n",
    "            d_loss_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "            \n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            \n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "\n",
    "            running_loss_D += d_loss.item()\n",
    "\n",
    "            # Train Generator min log(1 - D(G(z))) <-> max log(D(G(z)))\n",
    "\n",
    "            output = discriminator(fake_images).reshape(-1)\n",
    "            g_loss = criterion(output, torch.ones_like(output))\n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            running_loss_G += g_loss.item()\n",
    "\n",
    "        writer.add_scalar('Loss/Discriminator', running_loss_D / len(train_loader), epoch+1)\n",
    "        writer.add_scalar('Loss/Generator', running_loss_G / len(train_loader), epoch+1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake = generator(fixed_noise)\n",
    "            \n",
    "            img_batch = torch.cat((real_images[:4], fake), 0)\n",
    "            img_grid = make_grid(img_batch, nrow=img_batch.size(0) // 2)\n",
    "                    \n",
    "            writer.add_image('Generated vs Real', img_grid, global_step=epoch+1)\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class DoodlesDataset(Dataset):\n",
    "    def __init__(self, data, image_size=None, transform=None):\n",
    "        \n",
    "        self.images, self.labels = data\n",
    "        self.resize = T.Resize(image_size) if image_size else None\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = self.to_tensor(image)\n",
    "        \n",
    "        if self.resize:\n",
    "            image = self.resize(image)\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "def get_MNIST_data():\n",
    "    MNIST_train = datasets.MNIST(root='../data', train=True, download=True)\n",
    "    MNIST_val = datasets.MNIST(root='../data', train=False, download=True)\n",
    "\n",
    "    return MNIST_train, MNIST_val\n",
    "\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data, size=(32, 32), transform=None):\n",
    "        self.data = data\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.resize = T.Resize(size)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        image = self.to_tensor(image)\n",
    "        image = self.resize(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "train_data, val_data = get_MNIST_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (64, 64)\n",
    "CHANNELS_IN = 1\n",
    "HIDDEN_LAYERS_D = [128, 256, 512, 1024]\n",
    "HIDDEN_LAYERS_G = [1024, 512, 256, 128]\n",
    "LATENT_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "LEARNING_RATE_D = 2e-4\n",
    "LEARNING_RATE_G = 2e-4\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = 'mps'\n",
    "\n",
    "LOG_DIR = '../logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    # T.RandomHorizontalFlip(),\n",
    "    T.RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    T.Normalize((0.5, ), (0.5,))\n",
    "])\n",
    "# train_dataset = DoodlesDataset(train_data, image_size=IMAGE_SIZE)\n",
    "train_dataset = MNISTDataset(train_data, size=IMAGE_SIZE, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(DEVICE)\n",
    "\n",
    "discriminator = Discriminator(image_size=IMAGE_SIZE, channels_in=CHANNELS_IN, hidden_layers=HIDDEN_LAYERS_D)\n",
    "generator = Generator(hidden_layers=HIDDEN_LAYERS_G, channels_out=CHANNELS_IN, latent_dim=LATENT_DIM)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D, betas=(0.5, 0.999))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE_G, betas=(0.5, 0.999))\n",
    "\n",
    "train_gan(generator, discriminator, train_loader, optimizer_G, optimizer_D, \n",
    "          criterion, device, num_epochs=NUM_EPOCHS, latent_dim=LATENT_DIM, log_dir=LOG_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
