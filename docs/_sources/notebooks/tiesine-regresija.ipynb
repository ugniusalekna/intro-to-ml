{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tolydaus kintamojo prognozės uždavinys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sprendžiamas tolydaus kintamojo prognozės uždavinys, kurio tikslas – parinkti funkciją $f$ su kuria kuo tiksliau galėtume nustatyti kintamojo $y$ reikšmes pagal nepriklausomo kintamojo $x$ reikšmes, t.y. taip, kad lygybė $f(x) \\approx y$ būtų kiek įmanoma tikslesnė.\n",
    "\n",
    "- Dažniausias scenarijus – $y \\in \\mathbb{R}$, $x \\in \\mathbb{R}^{k}$, $k \\in \\mathbb{N}$. Tokiu atveju, vektorius žymėsime paryškintu šriftu, t.y., $\\textbf{x} := (x_1, x_2, ..., x_k)^T \\in \\mathbb{R}^{k}$, o skaliarus įprastai – $y := y \\in \\mathbb{R}$.\n",
    "\n",
    "- Porų $(\\textbf{x}^{(i)}, y^{(i)}), i = 1, ..., n$ imtis vadinama apmokymo aibe (*pastaba*: indeksai viršuje nėra laipsniai)\n",
    "\n",
    "- Funkcija $f$ MM kontekste vadinama hipoteze (angl. hypothesis), taigi, ji dažnai žymima raide $h$ (*pastaba*: MM hipotezė žymi visiškai kitą sąvoką nei statistikoje). Tolydaus kintamojo prognozės uždavinyje dar vadinama regresoriumi (angl. regressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiesinė regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tiesinė regresija yra vienas iš elementariausių mašininio mokymosi algoritmų, naudojamų tolydaus kintamojo prognozės uždaviniams.\n",
    "\n",
    "- Tiesinės regresijos tikslas – rasti geriausiai duomenų taškus atitinkančią tiesę (regresijos tiesę). Ši tiesė (tiesinės regresijos hipotezė) taške $\\textbf{x}$ apibrėžiama lygtimi:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_k x_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lygties parametrai $\\theta_1, \\theta_2, ..., \\theta_k$ vadinami krypties koeficientais (angl. coefficients), o parametras $\\theta_0$ – pastovus narys (angl. intercept).\n",
    "\n",
    "- Jeigu pažymime $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, ..., \\theta_k)^T$ ir $x_0 := 1$, tuomet nepriklausomų kintamųjų vektorių galime užrašyti kaip $\\textbf{x} = (x_0, x_1, ..., x_k)^T = (1, x_1, ..., x_k)^T \\in \\mathbb{R^{k+1}}$. Tada, tiesinės regresijos hipotezės taške $\\textbf{x}$ matricinis pavidalas yra:\n",
    "$$\n",
    "\\hat{y} = h(\\textbf{x}) = \\boldsymbol{\\theta}^T \\cdot \\textbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Siekiant atkreipti dėmesį, kad hipotezės funkcija $h$ priklauso ne tik nuo kintamųjų $\\textbf{x}$, bet ir nuo parametrų $\\boldsymbol{\\theta}$, dažnai žymime:\n",
    "$$\n",
    "h(\\textbf{x}) := h_{\\boldsymbol{\\theta}}(\\textbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Taigi, apsibrėžėme tiesinės regresijos hipotezę $h_{\\boldsymbol{\\theta}}$, tačiau kaip ją apmokyti? Šiuo atveju, modelio apmokymas reiškia geriausio parametrų rinkinio suradimą. Norint surasti geriausią parametrų rinkinį turime išmatuoti, kaip gerai (ar blogai) modelis tinka mokymo duomenims. Dažniausiai naudojamas tiesinės regresijos tikslumo matas yra standartinis nuokrypis (angl. root mean squared error, RMSE) – tai matas, nusakantis atsitiktinio dydžio įgyjamų reikšmių nuokrypį nuo vidurkio. Standartinis nuokrypis apibrėžiamas formule:\n",
    "\n",
    "$$\n",
    "\\text{RMSE}(\\textbf{x}, h_{\\boldsymbol{\\theta}}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(h_{\\boldsymbol{\\theta}}(\\textbf{x}^{(i)}) - y^{(i)} \\right)^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Norint išmokyti tiesinės regresijos modelį, reikia rasti tokią $\\boldsymbol{\\theta}$ vertę, kuri minimizuotų RMSE. Praktikoje paprasčiau minimizuoti vidutinę kvadratinę paklaidą (angl. mean squared error, MSE) nei RMSE, nes tai duoda tą patį rezultatą: \n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\textbf{x}, h_{\\boldsymbol{\\theta}}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\boldsymbol{\\theta}^T \\cdot \\textbf{x}^{(i)} - y^{(i)} \\right)^2\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
