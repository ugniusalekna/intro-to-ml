{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truputis mašininio mokymosi teorijos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kadangi šiek tiek susipažinome su prižiūrimo MM eiga, tolimesnėms temoms pravers šiek tiek statistinio mokymosi ir optimizavimo teorijos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hipotezė, nuostolių funkcija, rizika\n",
    "\n",
    "- Visų galimų požymių rinkinį žymėsime $\\mathcal{X} \\subseteq \\mathbb{R}^k$. Šio rinkinio elementai yra požymių vektoriai, pažymėti mažosiomis raidėmis $x$ (arba $\\textbf{x}$).\n",
    "\n",
    "- Reikšmių rinkinys bus žymimas $\\mathcal{Y}$ (baigtinė aibė klasifikavimo uždaviniuose, tolydi aibė regresijos uždaviniuose).\n",
    "\n",
    "- $\\mathcal{Z} = \\mathcal{X} \\times \\mathcal{Y}$ žymėsime stebinių aibę.\n",
    "\n",
    "- Bet kokia (mati) funkcija $h : \\mathcal{X} \\rightarrow \\mathcal{Y}$ vadinama hipoteze ir gali būti naudojama prognozavimui.\n",
    "\n",
    "- Visų galimų hipotezių rinkinį žymėsime $\\mathcal{Y}^{\\mathcal{X}}$. Paprastai nagrinėjame specifinius hipotezių rinkinius, o ne visą įmanomą hipotezių klasę (žymėsime $\\mathcal{H} \\subseteq \\mathcal{Y}^{\\mathcal{X}}$).\n",
    "\n",
    "- Siekiant pamatuoti nuostolį, kurį patiriame prognozei taške $x$ naudodami hipotezę $h$, t.y. prognozuodami būsimą $y$ reikšmę lygią $h(x)$, nagrinėjamai hipotezių klasei $\\mathcal{H}$ įvedama nuostolių funkcija $\\ell : \\mathcal{Z} \\times \\mathcal{H} \\rightarrow [ 0; \\infty )$. Reikšmė $l(z,h) = \\ell((x,y),h)$ žymi nuostolį, kurį patirsime tikrąją stebinio reikšmę $y$ keisdami prognoze $h(x)$.\n",
    "\n",
    "- $\\ell(z,h)$ kiekybiškai įvertina nuostolį konkrečiai stebinio reikšmei, tačiau kadangi stebinys yra atsitiktinis dydis, hipotezė $h$ turėtų būti pakankamai tiksli visoms galimoms jo reikšmėms.\n",
    "\n",
    "- Norint pasirinkti $h$ taip, kad lygybė $h(x) \\approx y$ būtų kiek įmanoma tikslesnė visoms $x$ reikšmėms, įvedama rizikos funkcija $R(h) = \\mathbb{E} [ \\ell(z, h) ]$. Vidurkis imamas pagal $z$, funkcijos $R$ apibrėžimo sritis yra nagrinėjamos hipotezių klasės $\\mathcal{H}$.\n",
    "\n",
    "- $R(h)$ matuoja vidutinį hipotezės $h$ nuostolį fiksuotos nuostolių funkcijos $l$ atžvilgiu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirinės rizikos minimizavimo taisyklė\n",
    "\n",
    "- Dėja, tačiau realiose situacijoje natūralu laikyti, kad atsitiktinio dydžio $z = (x,y)$ skirstinys yra nežinomas. Jeigu nežinome skirstinio, negalime apskaičiuoti jo vidurkio, t.y., dydžio $\\mathbb{E} [ \\ell(z, h) ]$. Taigi, pasirinkę nuostolių funkciją $\\ell$ negalime suskaičiuoti ją atitinkančios rizikos – negalime ieškoti optimalios hipotezės (rizikos funkcijos minimumo taško) $\\tilde{h} \\in \\text{arg} \\min_{h \\in \\mathcal{H}} R(h)$.\n",
    "\n",
    "- Siekiant rasti išeitį, apibrėžiama *empirinės rizikos funkcija*:\n",
    "\n",
    "$$\n",
    "R_{emp}(h) = \\frac{1}{n} \\sum_{i=1}^{n}{\\ell(h, z_i) },\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kur $(z_1, ..., z_n)$ – mokymosi duomenų aibė.\n",
    "\n",
    "- Remiantis **Didžiųjų skaičių dėsniu** (angl. the law of large numbers), artinant $n$ į begalybę, empirinės rizikos funkcija konverguoja į tikrąją riziką. Taigi, jei mokymosi duomenų aibės apimtis didelė, tai $R_{emp}(h) \\approx R(h)$. Tuo paremta empirinės rizikos minimizavimo taisyklė – minimizuodami $R_{emp}$ gausime hipotezę $h$, kurios tikra rizika bus artima jos minimumui (infimumui)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizavimo pagrindai\n",
    "\n",
    "- Optimizavimo uždavinys sprendžiamas taikant kone kiekvieną MM algoritmą (išimtis – tiesinės regresijos modelis, tačiau ir jo optimalūs parametrai gali būti ieškomi optimizavimo metodais).\n",
    "\n",
    "- Optimizavimas atliekamas tuomet, kai ieškome optimalios hipotezės $h$ iš turimos hipotezių klasės $\\mathcal{H}$. Pagrindinis optimizavimo uždavinys – rasti duotos funkcijos minimumo tašką, t.y. tašką, kuriame funkcija įgyja mažiausią reikšmę.\n",
    "\n",
    "- Tegul $\\mathcal{W} \\in \\mathbb{R}^k$, $f : \\mathcal{W} \\rightarrow \\mathbb{R}$ – duota funkcija. Uždavinys – rasti funkcijos $f$ minimumo tašką $w^{*}$, t.y. tašką iš aibės $\\text{arg} \\min_{w \\in \\mathcal{W}} f(w)$.\n",
    "\n",
    "### Sąvokos\n",
    "\n",
    "- *Globalaus minimumo taškas* – toks taškas $w^{*} \\in \\mathcal{W}$m kad $f(w^{*}) \\leq f(w)$ su visais $w \\in \\mathcal{W}$.\n",
    "\n",
    "- *Lokalaus minimumo taškas* – toks taškas $w^{*} \\in \\mathcal{W}$m kad $f(w^{*}) \\leq f(w)$ su visais $w$ iš tam tikros aplinkos, nelygios $\\mathcal{W}$.\n",
    "\n",
    "- *Funkcijos gradientas* – pirmų dalinių išvestinių vektorius. Gradientas taške $w$ paprastai žymimias $\\nabla f(w)$. Gradiento išraiška taške $w$:\n",
    "\n",
    "$$\n",
    "\\nabla f(w) := \\left (  \\frac{\\partial f}{\\partial w_1}(w), \\frac{\\partial f}{\\partial w_2}(w), ..., \\frac{\\partial f}{\\partial w_k}(w) \\right )^T.\n",
    "$$\n",
    "\n",
    "Funkcijos gradientas rodo greičiausio funkcijos didėjimo taške $w$ kryptį, tuo tarpu, antigradientas (neigiama reikšmė) – didžiausio nuolydžio taške $w$ kryptį. Tuo iš tiesų nėra sunku įsitikinti:\n",
    "\n",
    "Jeigu $u \\in \\mathcal{W} : || u || = 1$, tai kryptinė išvestinė $u \\cdot \\nabla f(w)$ rodo funkcijos $f$ didėjimo greitį $u$ kryptimi. Remiantis Koši-Švarco nelygybe skaliarinei sandaugai, galime įvertinti dydį:\n",
    "\n",
    "$$\n",
    "| u \\cdot \\nabla f(w) | \\leq || u || \\text{ } || \\nabla f(w) || = || \\nabla f(w) ||.\n",
    "$$\n",
    "\n",
    "Jei paimsime vienetinio ilgio vektorių gradiento kryptimi $ u_0 = \\frac{\\nabla f(w)}{|| \\nabla f(w) ||}$, gausime, jog\n",
    "\n",
    "$$\n",
    "u_0 \\cdot \\nabla f(w) = \\frac{\\nabla f(w) \\cdot \\nabla f(w) }{|| \\nabla f(w) ||} = || \\nabla f(w) ||, \n",
    "$$\n",
    "\n",
    "t.y. $\\nabla f(w)$ kryptis tikrai sutampa su didžiausio didėjimo kryptimi!\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "\n",
    "$\\square$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradientinio nusileidimo algoritmas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "**Algoritmas 1: Gradientinio nusileidimo algoritmas**\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\textbf{Require: } n \\geq 1, w_0 \\in \\mathcal{W}, \\epsilon > 0, s_0 > 0 \\cr\n",
    "&\\quad \\text{Inicializuojame maks. iteracijų sk., pradinį artinį, tikslumą, pradinį žingsnio dydį} \\cr\n",
    "&\\quad \\text{Set } m \\leftarrow 0 \\cr\n",
    "&\\quad \\text{Set } S \\leftarrow 0 \\cr\n",
    "&\\quad \\textbf{while } m \\leq n \\textbf{ do} \\cr\n",
    "&\\quad \\quad m \\leftarrow m + 1 \\cr\n",
    "&\\quad \\quad \\text{Perskaičiuojame } w_m = w_{m-1} - s_{m-1} \\nabla f(w_{m-1}) \\cr\n",
    "&\\quad \\quad \\text{Parenkame } s_m \\geq 0 \\text{ naujai iteracijai} \\cr\n",
    "&\\quad \\quad S \\leftarrow S + w_m \\cr\n",
    "&\\quad \\quad \\textbf{if } \\left| f(w_{m-1}) - f(w_m) \\right| < \\epsilon \\textbf{ then} \\cr\n",
    "&\\qquad \\quad \\text{Nutraukiame ciklą} \\cr\n",
    "&\\quad \\quad \\textbf{end if} \\cr\n",
    "&\\quad \\textbf{end while} \\cr\n",
    "&\\quad \\text{return } \\bar{w} = \\frac{S}{m}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
