{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truputis mašininio mokymosi teorijos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kadangi šiek tiek susipažinome su prižiūrimo MM eiga, tolimesnėms temoms pravers šiek tiek statistinio mokymosi ir optimizavimo teorijos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hipotezė, nuostolių funkcija, rizika\n",
    "\n",
    "- Visų galimų požymių rinkinį žymėsime $\\mathcal{X} \\subseteq \\mathbb{R}^k$. Šio rinkinio elementai yra požymių vektoriai, pažymėti mažosiomis raidėmis $x$ (arba $\\textbf{x}$).\n",
    "\n",
    "- Reikšmių rinkinys bus žymimas $\\mathcal{Y}$ (baigtinė aibė klasifikavimo uždaviniuose, tolydi aibė regresijos uždaviniuose).\n",
    "\n",
    "- $\\mathcal{Z} = \\mathcal{X} \\times \\mathcal{Y}$ žymėsime stebinių aibę.\n",
    "\n",
    "- Bet kokia (mati) funkcija $h : \\mathcal{X} \\rightarrow \\mathcal{Y}$ vadinama hipoteze ir gali būti naudojama prognozavimui.\n",
    "\n",
    "- Visų galimų hipotezių rinkinį žymėsime $\\mathcal{Y}^{\\mathcal{X}}$. Paprastai nagrinėjame specifinius hipotezių rinkinius, o ne visą įmanomą hipotezių klasę (žymėsime $\\mathcal{H} \\subseteq \\mathcal{Y}^{\\mathcal{X}}$).\n",
    "\n",
    "- Siekiant pamatuoti nuostolį, kurį patiriame prognozei taške $x$ naudodami hipotezę $h$, t.y. prognozuodami būsimą $y$ reikšmę lygią $h(x)$, nagrinėjamai hipotezių klasei $\\mathcal{H}$ įvedama nuostolių funkcija $\\ell : \\mathcal{Z} \\times \\mathcal{H} \\rightarrow [ 0; \\infty )$. Reikšmė $l(z,h) = \\ell((x,y),h)$ žymi nuostolį, kurį patirsime tikrąją stebinio reikšmę $y$ keisdami prognoze $h(x)$.\n",
    "\n",
    "- $\\ell(z,h)$ kiekybiškai įvertina nuostolį konkrečiai stebinio reikšmei, tačiau kadangi stebinys yra atsitiktinis dydis, hipotezė $h$ turėtų būti pakankamai tiksli visoms galimoms jo reikšmėms.\n",
    "\n",
    "- Norint pasirinkti $h$ taip, kad lygybė $h(x) \\approx y$ būtų kiek įmanoma tikslesnė visoms $x$ reikšmėms, įvedama rizikos funkcija $R(h) = \\mathbb{E} [ \\ell(z, h) ]$. Vidurkis imamas pagal $z$, funkcijos $R$ apibrėžimo sritis yra nagrinėjamos hipotezių klasės $\\mathcal{H}$.\n",
    "\n",
    "- $R(h)$ matuoja vidutinį hipotezės $h$ nuostolį fiksuotos nuostolių funkcijos $l$ atžvilgiu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirinės rizikos minimizavimo taisyklė\n",
    "\n",
    "- Dėja, tačiau realiose situacijoje natūralu laikyti, kad atsitiktinio dydžio $z = (x,y)$ skirstinys yra nežinomas. Jeigu nežinome skirstinio, negalime apskaičiuoti jo vidurkio, t.y., dydžio $\\mathbb{E} [ \\ell(z, h) ]$. Taigi, pasirinkę nuostolių funkciją $\\ell$ negalime suskaičiuoti ją atitinkančios rizikos – negalime ieškoti optimalios hipotezės (rizikos funkcijos minimumo taško) $\\tilde{h} \\in \\text{arg} \\min_{h \\in \\mathcal{H}} R(h)$.\n",
    "\n",
    "- Siekiant rasti išeitį, apibrėžiama *empirinės rizikos funkcija*:\n",
    "\n",
    "$$\n",
    "R_{emp}(h) = \\frac{1}{n} \\sum_{i=1}^{n}{\\ell(h, z_i) },\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kur $(z_1, ..., z_n)$ – mokymosi duomenų aibė\n",
    "\n",
    "- Remiantis **Didžiųjų skaičių dėsniu** (angl. the law of large numbers), artinant $n$ į begalybę, empirinės rizikos funkcija konverguoja į tikrąją riziką. Taigi, jei mokymosi duomenų aibės apimtis didelė, tai $R_{emp}(h) \\approx R(h)$. Tuo paremta empirinės rizikos minimizavimo taisyklė – minimizuodami $R_{emp}$ gausime hipotezę $h$, kurios tikra rizika bus artima jos minimumui (infimumui)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ell$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
