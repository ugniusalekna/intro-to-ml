{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generatyviniai neuroniniai tinklai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generatyviniai adversariniai tinklai (angl. generative adversarial networks, GANs) yra dar viena dirbtinių neuroninių tinklų architektūra, naudojama naujų duomenų (vaizdo, garso) generavimui. \n",
    "\n",
    "- Idėja buvo prisatyta 2014 m. mokslininko Ian Goodfellow ir jo kolegų."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_paper.png\" alt=\"gans-paper\" width=\"35%\">\n",
    "<p><strong>1.14 pav., Straipsnis, kuriame pristatyti GANs </strong></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iki GAN tinklų atsiradimo generatyviniai modeliai nė neegzistavo. Kai kurie eksperimentai buvo atliekami su autokoderiais, dažnai duodančiais neryškius vaizdus, artefaktus. \n",
    "\n",
    "- Bet žmonės jau žinojo, kaip sukurti galingus vaizdų klasifikatorius! 2012 m. pasirodęs *AlexNet* modelis buvo pirmasis modelis, sutriuškinęs savo konkurentus vaizdo klasifikavimo uždaviniuose. \n",
    "\n",
    "- Goodfellow idėja buvo, užuot kūrus galingą generatorių, paimti jau egzistuojančio klasifikatoriaus (kurį jis pavadino diskriminatoriumi) architektūrą bei jį panaudoti *apmokant kitą modelį*, atsakingą už naujų duomenų generavimą.\n",
    "\n",
    "- Pagrindinė GAN inovacija – generatoriaus užduotis ne tiesiogiai sukurti duomenis, panašius į apmokymo aibę (labai sunki užduotis), bet sukurti tokius duomenis, kurie galėtų apgauti diskriminatorių, klasifikuojant juos kaip tikrus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagrindiniai komponentai\n",
    "\n",
    "**Generatorius (G)**: Generatoriaus tinklo tikslas - sukurti duomenų pavyzdžius, kurie nesiskirtų nuo tikrų duomenų. Jis pradeda su atsitiktiniu triukšmo vektoriumi ir paverčia jį duomenų pavyzdžiu. Generatoriaus tikslas - sukurti išvestį, kuri būtų kuo artimesnė tikrajam duomenų pasiskirstymui.\n",
    "\n",
    "**Diskriminatorius (D)**: Diskriminatorius yra binarinis klasifikatorius, kuriuo bandoma atskirti tikruosius duomenų pavyzdžius nuo generatoriaus sukurtų pavyzdžių. Jo įvestis tikras arba sugeneruotas duomenų rinkinio pavyzdys, o išvestis – tikimybė, nurodanti, ar pavyzdys yra klasifikuojamsa kaip tikras, ar kaip netikras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarinio mokymosi procesas\n",
    "\n",
    "- GAN tinklo apmokymas yra minmax optimizavimo uždavinys tarp generatoriaus ir diskriminatoriaus\n",
    "\n",
    "- Mokymosi pradžioje generatorius pateikia akivaizdžiai netikrus duomenis, todėls diskriminatorius greitai išmoksta nustatyti, kad jie yra netikri\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_1.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- Tęsiantis mokymosi procesui, generatorius vis labiau artėja prie sugeneruotų duomenų, galinčių apgauti diskriminatorių.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_2.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- Galiausiai, jei generatoriaus apmokymas pavyksta, diskriminatorius vis prasčiau atskiria tikrą duomenų atvejį nuo netikro. Jis pradeda klasifikuoti netikrus duomenis kaip tikrus, ir jo tikslumas mažėja.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_3.png\" alt=\"gans-example\" width=\"85%\">\n",
    "</div>\n",
    "\n",
    "- GAN architektūros schema atrodo daugmaž taip:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/gans_architecture.png\" alt=\"gans-architecture\" width=\"80%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netikties funkcija\n",
    "\n",
    "- Straipsnyje, kuriame pristatyti GANs, netikties funkcija apibrėžiama formule:\n",
    "\n",
    "$$\n",
    "L(G, D) = \\frac{1}{m} \\sum_{i=1}^{m} [\\log D(x^{(i)})] + \\frac{1}{m} \\sum_{i=1}^{m} [\\log(1 - D(G(z^{(i)})))] \n",
    "$$\n",
    "\n",
    "- Generatorius ($G$) stengiasi minimizuoti šią funkciją, o diskriminatorius ($D$) stengiasi ją maksimizuoti (nes norime \"apgauti\" diskriminatorių). Tai yra kiek kitokia optimizavimo forma, nuo mums įprastos $\\min_{w} f(w)$:\n",
    "\n",
    "$$\n",
    "\\min_{G} \\max_{D} L(G, D)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daugdaros didelio matmens erdvėse\n",
    "\n",
    "- Kiekvieną vaizdą galime įsivaizduoti kaip tašką didelio matmens erdvėje – kiekvieno pikselio vertė atitinka tašką ant atitinkamos dimensijos ašies. Pvz., turėdami vieno kanalo (*grayscale*) nuotrauką, susidedančią iš 3 pikselių, šių nuotraukų erdvę galime geometriškai pavaizduoti 3D kubu:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/manifold_hypercube.gif\" alt=\"manifold-hypercube\" width=\"65%\">\n",
    "</div>\n",
    "\n",
    "- Tuo tarpu, 256x256 vieno kanalo vaizdą galima pavaizduoti 65 536 matmenų hiperkube.\n",
    "\n",
    "- Didžioji dauguma tokio hiperkubo taškų yra triukšmingi, beprasmiai vaizdai. Reikšmingi vaizdai, pavyzdžiui, nuotraukos ar parašyti puslapiai, šioje erdvėje pasitaiko itin retai.\n",
    "\n",
    "- Daugdaros – tai mažesnio matavimo poerdviai aukšto matmens erdvėse, turintys mažiau laisvės laipsnių (t.y. gali būti atvaizduoti į mažesnio matavimo erdves). \n",
    "\n",
    "- Manoma, jog prasmingi vaizdai yra išsidėstę mažesnio matmens poerdviuose šioje didelio matmens erdvėje (hiperkube). Pavyzdžiui, vaizdų, kuriuose vaizduojamas žmogaus veidas su skirtingomis išraiškomis, rinkinys yra kažkur „veidų daugdaroje“, kadangi visų tokių nuotraukų pikselių pasiskirstymai turėtų būti bent kiek panašūs. Suradus tokias daugdaras bei judant jomis, galime matyti sklandžiai (tolydžiai) besikeičiančių vaizdų animacijas.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/manifold_transitions.gif\" alt=\"manifold-transitions\" width=\"65%\">\n",
    "</div>\n",
    "\n",
    "- Generatyviniai adversariniai tinklai išmoksta aproksimuoti šias daugdaras. Jie atvaizduoja mažo matmens erdvę (angl. latent space) į didelio matmens vaizdų erdvę, taip išmokdami daugdaros struktūrą. Turint daugdaros aproksimaciją, ją galima panaudoti \"vaikštant joje\", taip sugeneruojant realistiškai atrodančias nuotraukas, esančias šalia viena kitos (toje *latent space*) bei gaunant gražius netriukšmingus perėjimus.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/drag_gan.gif\" alt=\"drag-gan\" width=\"65%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacija PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bicycle drawings\n",
      "load complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from quickdraw import QuickDrawDataGroup\n",
    "\n",
    "def load_quickdraw_data(classes, image_size, val_split=None, max_drawings_per_class=None):\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_dict = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        qdg = QuickDrawDataGroup(cls, max_drawings=max_drawings_per_class)\n",
    "        for drawing in qdg.drawings:\n",
    "            image = drawing.get_image().convert('L')\n",
    "            image = image.resize(image_size)\n",
    "            image_array = np.array(image)\n",
    "            image_array = 255 - image_array\n",
    "            images.append(image_array)\n",
    "            labels.append(label_dict[cls])\n",
    "\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    if val_split is not None:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, stratify=y, random_state=42)\n",
    "        X, y = (X_train, y_train), (X_val, y_val)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "CLASSES = [\n",
    "    \"bicycle\",\n",
    "]\n",
    "\n",
    "train_data = load_quickdraw_data(CLASSES, image_size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, activation=True, batch_norm=True, **kwargs):\n",
    "        layers = [nn.Conv2d(channels_in, channels_out, **kwargs)]\n",
    "        layers += [nn.BatchNorm2d(channels_out)] if batch_norm else []\n",
    "        layers += [nn.GELU()] if activation else []\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, activation=True, dropout=0.0, **kwargs):\n",
    "        layers = [nn.Linear(channels_in, channels_out, **kwargs)]\n",
    "        layers += [nn.GELU()] if activation else []\n",
    "        layers += [nn.Dropout(dropout)] if dropout > 0.0 else []\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, hidden_layers):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.channels_in = channels_in\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        conv_layers = []\n",
    "        \n",
    "        for i, channels_out in enumerate(hidden_layers):\n",
    "            stride = 2 if i == 0 or hidden_layers[i] != hidden_layers[i-1] else 1\n",
    "            conv_layers.append(\n",
    "                ConvBlock(channels_in, channels_out, kernel_size=3, padding=1, stride=stride)\n",
    "            )\n",
    "            channels_in = channels_out\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(*conv_layers)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "        conv_output_size = self._get_conv_output_size(*image_size)\n",
    "        \n",
    "        self.fc_blocks = nn.Sequential(\n",
    "            LinearBlock(conv_output_size, 64, dropout=0.25),\n",
    "            LinearBlock(64, 1, activation=False)\n",
    "        )\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _get_conv_output_size(self, height, width):\n",
    "        self.eval()\n",
    "        dummy_input = torch.zeros(1, 1, height, width)\n",
    "        output = self.conv_blocks(dummy_input)\n",
    "        self.train()\n",
    "        return output.numel()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_blocks(x)\n",
    "        \n",
    "        return F.sigmoid(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = Discriminator(image_size=(64, 64), channels_in=1, hidden_layers=[16, 32, 64])\n",
    "inp = torch.rand(1, 1, 64, 64, dtype=torch.float32)\n",
    "out = model(inp)\n",
    "\n",
    "print(inp.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 1, 1])\n",
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeconvBlock(nn.Sequential):\n",
    "    def __init__(self, channels_in, channels_out, activation=True, batch_norm=True, **kwargs):\n",
    "        layers = [nn.ConvTranspose2d(channels_in, channels_out, **kwargs)]\n",
    "        layers += [nn.BatchNorm2d(channels_out)] if batch_norm else []\n",
    "        layers += [nn.GELU()] if activation else []\n",
    "        super().__init__(*layers) \n",
    "        \n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_layers, channels_out, latent_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.channels_out = channels_out\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        deconv_blocks = [DeconvBlock(latent_dim, hidden_layers[0], kernel_size=4, stride=1, padding=0)]\n",
    "        deconv_blocks += [\n",
    "            DeconvBlock(hidden_layers[i], hidden_layers[i+1], kernel_size=4, stride=2, padding=1)\n",
    "            for i in range(len(hidden_layers) - 1)\n",
    "        ]\n",
    "        deconv_blocks += [DeconvBlock(hidden_layers[-1], channels_out, kernel_size=4, stride=2, padding=1,\n",
    "                                      activation=False)]\n",
    "        \n",
    "        self.deconv_blocks = nn.Sequential(*deconv_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv_blocks(x)\n",
    "        return F.tanh(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = Generator(hidden_layers=[1024, 512, 256, 128], channels_out=1, latent_dim=100)\n",
    "inp = torch.randn(1, 100, 1, 1, dtype=torch.float32)\n",
    "out = model(inp)\n",
    "\n",
    "print(inp.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, train_loader, optimizer_G, optimizer_D, \n",
    "              criterion, device, num_epochs=100, latent_dim=100, log_dir=\"../logs/\"):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "    writer = SummaryWriter(log_dir=log_dir + timestamp)\n",
    "\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "            \n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        running_loss_G = running_loss_D = 0.0\n",
    "\n",
    "        for real_images, _ in tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]', leave=False):\n",
    "            real_images = real_images.to(device)\n",
    "            batch_size = real_images.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            z = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "\n",
    "            # Discriminator loss\n",
    "            real_labels = torch.ones(batch_size, 1, device=device)\n",
    "            fake_labels = torch.zeros(batch_size, 1, device=device) # 1 - real\n",
    "\n",
    "            d_loss_real = criterion(discriminator(real_images), real_labels)\n",
    "            d_loss_fake = criterion(discriminator(generator(z).detach()), fake_labels)\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            running_loss_D += d_loss.item()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generator loss\n",
    "            g_loss = criterion(discriminator(generator(z)), real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            running_loss_G += g_loss.item()\n",
    "\n",
    "        writer.add_scalar('Loss/Discriminator', running_loss_D / len(train_loader), epoch+1)\n",
    "        writer.add_scalar('Loss/Generator', running_loss_G / len(train_loader), epoch+1)\n",
    "\n",
    "        img_batch = torch.cat((real_images[:1], generator(z).detach()[:1]), 0)\n",
    "        img_grid = make_grid(img_batch, nrow=img_batch.size(0) // 2)\n",
    "        \n",
    "        writer.add_image('Generated vs Real', img_grid, global_step=epoch+1)\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class DoodlesDataset(Dataset):\n",
    "    def __init__(self, data, image_size=None, transform=None):\n",
    "        \n",
    "        self.images, self.labels = data\n",
    "        self.resize = T.Resize(image_size) if image_size else None\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = self.to_tensor(image)\n",
    "        \n",
    "        if self.resize:\n",
    "            image = self.resize(image)\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (64, 64)\n",
    "CHANNELS_IN = 1\n",
    "HIDDEN_LAYERS_D = [16, 32]\n",
    "HIDDEN_LAYERS_G = [1024, 512, 256, 128]\n",
    "LATENT_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE_D = 2e-4\n",
    "LEARNING_RATE_G = 1e-4\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = 'mps'\n",
    "\n",
    "LOG_DIR = '../logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Normalize((0.5, ), (0.5,))\n",
    "])\n",
    "train_dataset = DoodlesDataset(train_data, image_size=IMAGE_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [64, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer_D \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(discriminator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE_D, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.999\u001b[39m))\n\u001b[1;32m      8\u001b[0m optimizer_G \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(generator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE_G, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.999\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_G\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLATENT_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOG_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 38\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, train_loader, optimizer_G, optimizer_D, criterion, device, num_epochs, latent_dim, log_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;66;03m# 1 - real\u001b[39;00m\n\u001b[1;32m     37\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m criterion(discriminator(real_images), real_labels)\n\u001b[0;32m---> 38\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m criterion(discriminator(\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()), fake_labels)\n\u001b[1;32m     40\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m d_loss_real \u001b[38;5;241m+\u001b[39m d_loss_fake\n\u001b[1;32m     41\u001b[0m d_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 30\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mtanh(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/container.py:249\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/container.py:249\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/conv.py:1150\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1139\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1140\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1142\u001b[0m     output_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m )\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [64, 100]"
     ]
    }
   ],
   "source": [
    "device = torch.device(DEVICE)\n",
    "\n",
    "discriminator = Discriminator(image_size=IMAGE_SIZE, channels_in=CHANNELS_IN, hidden_layers=HIDDEN_LAYERS_D)\n",
    "generator = Generator(hidden_layers=HIDDEN_LAYERS_G, channels_out=CHANNELS_IN, latent_dim=LATENT_DIM)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D, betas=(0.5, 0.999))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE_G, betas=(0.5, 0.999))\n",
    "\n",
    "train_gan(generator, discriminator, train_loader, optimizer_G, optimizer_D, \n",
    "          criterion, device, num_epochs=NUM_EPOCHS, latent_dim=LATENT_DIM, log_dir=LOG_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
