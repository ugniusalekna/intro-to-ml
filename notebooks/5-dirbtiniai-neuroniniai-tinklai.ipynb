{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirbtiniai neuroniniai tinklai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dirbtiniai neuroniniai tinklai (angl. artificial neural networks, ANNs) yra MM modelių klasė, įkvėpta smegenų neuronų architektūra. \n",
    "\n",
    "- Dirbtiniai neuroniniai tinklai egzistuoja jau gana seniai – pirmą kartą juos 1943 m. pristatė neurofiziologas Warrenas McCullochas ir matematikas Walteris Pittsas. Jie buvo sukurti kaip būdas aproksimuoti sudėtingas funkcijas, neturinčias aiškios išreikštinės formos.\n",
    "\n",
    "- Dirbtinius neuroninius tinklus sudaro sujungti mazgai, vadinami dirbtiniais neuronais. Juos jungia briaunos, atitinkančios smegenų sinapses. Kiekvienas dirbtinis neuronas gauna signalus iš prieš jį esančių neuronų, tada juos apdoroja ir siunčia signalą toliau kitiems neuronams. Signalas yra realusis skaičius, o kiekvieno neurono išvestis apskaičiuojama pagal tam tikrą taisyklę. Signalo stiprumą kiekviename ryšyje lemia svoris, dažniausiai pavaizduotas ant briaunos.\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/real_neuron.png\" alt=\"real-neuron\" width=\"65%\">\n",
    "<p><strong>1.8 pav., Biologinis neuronas </strong></p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 2px; background-color: black;\">\n",
    "\n",
    "- Dirbtiniai neuroniniai tinklai taikomi tiek regresijos, tiek klasifikavimo uždaviniuose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matematinė formuluotė "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tegul $x = (x_1, \\ldots, x_d) \\in \\mathbb{R}^{d_0}$. Dirbtinis neuroninis tinklas $h^L : \\mathbb{R}^{d_0} \\rightarrow \\mathbb{R}^d$ [čia $d = d_L$] apibrėžiamas kaip funkcijų $ h^l = (h_1^l, \\ldots, h_{d_l}^l): \\mathbb{R}^{d_{l-1}} \\rightarrow \\mathbb{R}^{d_l}, \\text{ } l = 1, \\ldots, L $ kompozicija, kur $h^l$ komponentės užrašomos\n",
    "\n",
    "$$\n",
    "h_j^1(x) = \\sum_{i=1}^{d_0} w_{ij}^1 x_i + b_j^1, \\quad j = 1, \\ldots, d_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_j^l(x) = \\sum_{i=1}^{d_{l-1}} w_{ij}^l \\sigma(h_i^{l-1}(x)) + b_j^l, \\quad j = 1, \\ldots, d_l \\quad l = 2, \\ldots, L\n",
    "$$\n",
    "\n",
    "- Čia $L$ žymi dirbtinio neuroninio tinklo *sluoksnių skaičių*. \n",
    "- Kiekvienas sluoksnis $l = 1, \\ldots, L$ turi $d_l$ dirbtinių neuronų, $W := \\max_{1 \\leq l \\leq L} d_l$ žymi neuroninio tinklo *plotį*. \n",
    "- Koeficientai $w_{ij}^l$ atitinka *svorius*, kurie jungia $i$-ąjį neuroną $(l-1)$-ajame sluoksnyje su $j$-uoju neuronu $l$-ajame sluoksnyje; $b_j^l$ yra $j$-ojo neurono $l$-ajame sluoksnyje *laisvasis narys*. \n",
    "- Funkcija $\\sigma$, istoriškai vadinama *aktyvacijos funkcija*, yra žinoma ir fiksuota, tuo tarpu koeficientai $w_{ij}^l$ ir laisvasis narys $b_j^l$ – nežinomi. \n",
    "\n",
    "- MM kontekste, paskutinio sluoksnio išvestis, žymima $h^L$, yra hipotezės funkcija. Visų dirbtinio neuroninio tinklo svorių ir laisvųjų narių rinkinys vadinamas tinklo *parametrais* $\\theta$. Ši parametrų aibė apibrėžia dirbtinio neuroninio tinklo architektūrą, todėl kartais žymėsime $h^L(x) = h^L(x; \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pažymėkime apmokymo aibę $D = \\{ (x^i, y^i) \\mid i = 1, \\ldots, N\\}$. Tam, kad galėtume kiekybiškai įvertinti nuostolį vienai stebinio reikšmei $(x^i, y^i)$, įvedama nuostolių funkcija $\\ell$. Norint, kad $h^L$ artinys būtų geras visoje aibėje $D$, įvedama empirinės rizikos funkcija:\n",
    "\n",
    "$$\n",
    "    R_{emp}(h^L) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left( (x^i, y^i), h^L(x^i;\\theta) \\right).\n",
    "$$\n",
    "\n",
    "- Dalykai jau pažįstami iš anksčiau :)\n",
    "\n",
    "*pastaba:* literatūroje empirinės rizikos funkcija dažniausiai vadinama tiesiog netiktimi, sutapatinant du terminus į vieną."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuroniniai tinklai apmokomi minimizuojant empirinės rizikos funkciją. Minimizavimas dažniausiai atliekamas gradientinio nusileidimo metodais. Taikant šiuos metodus, reikia apskaičiuoti empirinės rizikos funkcijos gradientą. \n",
    "\n",
    "1986 m. Davidas Rumelhartas, Geoffrey Hintonas ir Ronaldas Williamsas išleido straipsnį, kuriame buvo pristatytas sklidimo atgal apmokymo algoritmas (angl. backpropagation), kuris naudojamas iki šiol.\n",
    "\n",
    "Algoritmas yra efektyvus, nes tik per du perėjimus per tinklą (vieną pirmyn, kitą atgal) galima apskaičiuoti empirinės rizikos funkcijos gradientą kiekvieno modelio parametro atžvilgiu.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/ugniusalekna/intro-to-ml/main/images/backprop.png\" alt=\"backprop\" width=\"65%\">\n",
    "<p><strong>1.9 pav., Sklidimo atgal algoritmas </strong></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
